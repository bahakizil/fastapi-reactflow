# Flowise FastAPI Backend

A powerful, modular FastAPI-based backend for building AI/LLM workflows with a visual interface, inspired by Flowise but built with a focus on developer experience and extensibility.

## üöÄ Features

- **Standardized Node Architecture**: Three-tier node system (Provider, Processor, Terminator)
- **Dynamic Node Discovery**: Automatic detection and registration of new nodes
- **LangChain Integration**: Full compatibility with LangChain ecosystem
- **Modular Design**: Easy to extend with custom nodes
- **Type-Safe**: Built with Pydantic for robust data validation
- **RESTful API**: Clean, documented endpoints for frontend integration

## üìÅ Project Structure

```
flowise-fastapi/
‚îú‚îÄ‚îÄ api/                    # FastAPI routes and schemas
‚îÇ   ‚îú‚îÄ‚îÄ routers/           # API route handlers
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py         # Pydantic models
‚îú‚îÄ‚îÄ core/                  # Core system components
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ node_discovery.py  # Automatic node discovery
‚îÇ   ‚îî‚îÄ‚îÄ workflow_runner.py # Workflow execution engine
‚îú‚îÄ‚îÄ nodes/                 # Node implementations
‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Base node classes
‚îÇ   ‚îú‚îÄ‚îÄ llms/             # Language models
‚îÇ   ‚îú‚îÄ‚îÄ tools/            # External tools
‚îÇ   ‚îú‚îÄ‚îÄ agents/           # AI agents
‚îÇ   ‚îú‚îÄ‚îÄ memory/           # Conversation memory
‚îÇ   ‚îú‚îÄ‚îÄ prompts/          # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ output_parsers/   # Output formatting
‚îÇ   ‚îú‚îÄ‚îÄ document_loaders/ # Document processing
‚îÇ   ‚îî‚îÄ‚îÄ retrievers/       # Information retrieval
‚îú‚îÄ‚îÄ main.py               # FastAPI application entry point
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îî‚îÄ‚îÄ readme-gemini.md      # Detailed project memory/documentation
```



3. **Set up environment variables:**
   ```bash
   export OPENAI_API_KEY="your-openai-api-key"
   export GOOGLE_API_KEY="your-google-api-key"  # Optional, for Gemini
   export TAVILY_API_KEY="your-tavily-api-key"  # Optional, for Tavily search
   export LANGCHAIN_API_KEY="your-langchain-api-key"  # Optional, for LangSmith tracing
   ```

## üöÄ Quick Start

1. **Start the server:**
   ```bash
   uvicorn main:app --reload
   ```

2. **Visit the API documentation:**
   - Interactive docs: http://localhost:8000/docs
   - ReDoc: http://localhost:8000/redoc

3. **List available nodes:**
   ```bash
   curl http://localhost:8000/api/v1/nodes
   ```

## üìä Available Nodes

### Provider Nodes (Create LangChain objects)
- **OpenAIChat**: OpenAI GPT models
- **TavilySearch**: Web search via Tavily API
- **PromptTemplate**: Chat prompt templates
- **AgentPrompt**: ReAct agent prompts
- **ConversationMemory**: Conversation buffer memory
- **PDFLoader**: PDF document loader

### Processor Nodes (Combine multiple objects)
- **ReactAgent**: ReAct reasoning agent
- **ChromaRetriever**: Vector store retrieval

### Terminator Nodes (Process outputs)
- **StringOutputParser**: Basic string output
- **PydanticOutputParser**: Structured output parsing

## üîß API Endpoints

### Nodes
- `GET /api/v1/nodes` - List all available nodes
- `GET /api/v1/nodes/{node_name}` - Get specific node details

### Workflows
- `POST /api/v1/workflows/execute` - Execute a workflow

### Example Workflow Execution
```json
{
  "workflow": {
    "id": "example-workflow",
    "name": "Simple Chat",
    "nodes": [
      {
        "id": "llm-1",
        "type": "OpenAIChat",
        "data": {
          "inputs": {
            "model_name": "gpt-4o-mini",
            "temperature": 0.7
          }
        }
      }
    ],
    "edges": []
  },
  "input": {
    "input": "Hello, how are you?"
  }
}
```



1. **Provider Node Example:**
```python
from nodes.base import ProviderNode, NodeInput, NodeType
from langchain_core.runnables import Runnable

class MyCustomNode(ProviderNode):
    _metadatas = {
        "name": "MyCustomNode",
        "description": "Description of what this node does",
        "node_type": NodeType.PROVIDER,
        "inputs": [
            NodeInput(
                name="param1",
                type="string",
                description="Parameter description",
                required=True
            )
        ]
    }
    
    def _execute(self, param1: str = None) -> Runnable:
        # Your implementation here
        return your_langchain_object
```

2. **Place the file in the appropriate `nodes/` subdirectory**
3. **Restart the server** - the node will be automatically discovered

## üîç Node Types

- **ProviderNode**: Creates LangChain objects from scratch (LLMs, Tools, Prompts)
- **ProcessorNode**: Combines multiple LangChain objects (Agents, Chains)
- **TerminatorNode**: Processes final outputs (Parsers, Formatters)
